{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2d1fb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80606c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy opencv-python matplotlib scikit-learn imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5096dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA is available but not initialized properly\n"
     ]
    }
   ],
   "source": [
    "import random, os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available() and not torch.cuda.is_initialized():\n",
    "        print(\"CUDA is available but not initialized properly\")\n",
    "    else:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f65147",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3669c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-net based architecture\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNetBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNetBinaryClassifier_CE(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_channels=32):\n",
    "        super(UNetBinaryClassifier_CE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = UNetBlock(in_channels, base_channels)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = UNetBlock(base_channels, base_channels * 2)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = UNetBlock(base_channels * 2, base_channels * 4)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.enc4 = UNetBlock(base_channels * 4, base_channels * 8)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = UNetBlock(base_channels * 8, base_channels * 16)\n",
    "\n",
    "        # Decoder\n",
    "        self.up4 = nn.ConvTranspose2d(\n",
    "            base_channels * 16, base_channels * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.dec4 = UNetBlock(base_channels * 16, base_channels * 8)\n",
    "        self.up3 = nn.ConvTranspose2d(\n",
    "            base_channels * 8, base_channels * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.dec3 = UNetBlock(base_channels * 8, base_channels * 4)\n",
    "        self.up2 = nn.ConvTranspose2d(\n",
    "            base_channels * 4, base_channels * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.dec2 = UNetBlock(base_channels * 4, base_channels * 2)\n",
    "        self.up1 = nn.ConvTranspose2d(\n",
    "            base_channels * 2, base_channels, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.dec1 = UNetBlock(base_channels * 2, base_channels)\n",
    "\n",
    "        # Final classifier (global average pooling + linear layer)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(base_channels, 2),  # Output 2 logits for binary classification\n",
    "            # Removed nn.Sigmoid() because CrossEntropyLoss expects raw logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "        e4 = self.enc4(self.pool3(e3))\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool4(e4))\n",
    "\n",
    "        # Decoder\n",
    "        d4 = self.dec4(torch.cat([self.up4(b), e4], dim=1))\n",
    "        d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "\n",
    "        # Global pooling + classification\n",
    "        out = self.global_pool(d1)\n",
    "        out = self.classifier(out)  # [batch_size, 2], raw logits\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-net based architecture (AAM loss)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class UNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNetBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class ArcFaceLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50):\n",
    "        super(ArcFaceLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, embeddings, labels=None):\n",
    "        # Normalize embeddings and weights\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)  # [batch, in_features]\n",
    "        weights = F.normalize(self.weight, p=2, dim=1)  # [out_features, in_features]\n",
    "\n",
    "        # Cosine similarity\n",
    "        cos_theta = F.linear(embeddings, weights)  # [batch, out_features]\n",
    "\n",
    "        if labels is None:\n",
    "            # Inference: return scaled cosine similarities\n",
    "            return self.s * cos_theta\n",
    "\n",
    "        # Training: apply ArcFace margin\n",
    "        theta = torch.acos(torch.clamp(cos_theta, -1.0 + 1e-7, 1.0 - 1e-7))\n",
    "        one_hot = torch.zeros_like(cos_theta)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1).long(), 1)\n",
    "\n",
    "        # Add margin to true class\n",
    "        cos_theta_m = torch.cos(theta + self.m * one_hot)\n",
    "        logits = self.s * (cos_theta * (1.0 - one_hot) + cos_theta_m * one_hot)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class UNetBinaryClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels=3, base_channels=32, embedding_size=512, num_classes=2\n",
    "    ):\n",
    "        super(UNetBinaryClassifier, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = UNetBlock(in_channels, base_channels)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.enc2 = UNetBlock(base_channels, base_channels * 2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.enc3 = UNetBlock(base_channels * 2, base_channels * 4)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.enc4 = UNetBlock(base_channels * 4, base_channels * 8)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = UNetBlock(base_channels * 8, base_channels * 16)\n",
    "\n",
    "        # Decoder\n",
    "        self.up4 = nn.ConvTranspose2d(\n",
    "            base_channels * 16, base_channels * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.dec4 = UNetBlock(base_channels * 16, base_channels * 8)\n",
    "        self.up3 = nn.ConvTranspose2d(\n",
    "            base_channels * 8, base_channels * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.dec3 = UNetBlock(base_channels * 8, base_channels * 4)\n",
    "        self.up2 = nn.ConvTranspose2d(\n",
    "            base_channels * 4, base_channels * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.dec2 = UNetBlock(base_channels * 4, base_channels * 2)\n",
    "        self.up1 = nn.ConvTranspose2d(\n",
    "            base_channels * 2, base_channels, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.dec1 = UNetBlock(base_channels * 2, base_channels)\n",
    "\n",
    "        # Embedding layer\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(base_channels, embedding_size),\n",
    "            nn.BatchNorm1d(embedding_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # ArcFace layer\n",
    "        self.arcface = ArcFaceLayer(embedding_size, num_classes, s=30.0, m=0.5)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "        e4 = self.enc4(self.pool3(e3))\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool4(e4))\n",
    "\n",
    "        # Decoder\n",
    "        d4 = self.dec4(torch.cat([self.up4(b), e4], dim=1))\n",
    "        d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "\n",
    "        # Embedding\n",
    "        emb = self.embedding(self.global_pool(d1))  # [batch, embedding_size]\n",
    "\n",
    "        # ArcFace logits\n",
    "        logits = self.arcface(emb, labels)  # [batch, num_classes]\n",
    "\n",
    "        # Probabilities for inference\n",
    "        probs = F.softmax(logits, dim=1) if labels is None else None\n",
    "\n",
    "        return logits, probs, emb\n",
    "\n",
    "    def get_activation_map(self, x):\n",
    "        with torch.no_grad():\n",
    "            e1 = self.enc1(x)\n",
    "            e2 = self.enc2(self.pool1(e1))\n",
    "            e3 = self.enc3(self.pool2(e2))\n",
    "            e4 = self.enc4(self.pool3(e3))\n",
    "            b = self.bottleneck(self.pool4(e4))\n",
    "            d4 = self.dec4(torch.cat([self.up4(b), e4], dim=1))\n",
    "            d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1))\n",
    "            d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "            d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        return d1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1d4a23",
   "metadata": {},
   "source": [
    "# AAM loss training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3bfecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# === Define Paths ===\n",
    "root_folder = \"project_path\\replay attack\"\n",
    "\n",
    "# Paths for Caffe face detector\n",
    "protoPath = os.path.join(root_folder, \"face_detector/deploy.prototxt.txt\")\n",
    "modelPath = os.path.join(\n",
    "    root_folder, \"face_detector/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    ")\n",
    "\n",
    "face_confidence = 0.75  # Confidence threshold for face detection\n",
    "\n",
    "# Root directory for extracted Replay-Attack data\n",
    "Replay_Attack_extracted = os.path.join(\n",
    "    root_folder, \"dataset/training/Replay-Attack extracted/\"\n",
    ")\n",
    "\n",
    "# === Output Directories ===\n",
    "# Output directories for detected faces (training and validation)\n",
    "output_attack_training_folder = os.path.join(\n",
    "    root_folder, \"dataset/training/Replay_Attack/images/attack_training/\"\n",
    ")\n",
    "output_attack_validation_folder = os.path.join(\n",
    "    root_folder, \"dataset/training/Replay_Attack/images/attack_validation/\"\n",
    ")\n",
    "output_bonifade_training_folder = os.path.join(\n",
    "    root_folder, \"dataset/training/Replay_Attack/images/bonifade_training/\"\n",
    ")\n",
    "output_bonifade_validation_folder = os.path.join(\n",
    "    root_folder, \"dataset/training/Replay_Attack/images/bonifade_validation/\"\n",
    ")\n",
    "\n",
    "# Directory for augmented images (if used during training)\n",
    "save_augmented_images = os.path.join(\n",
    "    root_folder, \"dataset/training/Replay_Attack/augmented_images/\"\n",
    ")\n",
    "\n",
    "# Paths for saving model checkpoint, labels, metrics plot, and training array\n",
    "save_model_pth = os.path.join(\n",
    "    root_folder, \"dataset/arcface/best_model_Replay_Attack_arcface.pth\"\n",
    ")\n",
    "save_labels = os.path.join(\n",
    "    root_folder, \"dataset/arcface/model_Replay_Attack_labels_arcface.csv\"\n",
    ")\n",
    "save_training_metrics_plot = os.path.join(\n",
    "    root_folder, \"dataset/arcface/Replay_Attack_plot_arcface.png\"\n",
    ")\n",
    "output_np_training_array = os.path.join(\n",
    "    root_folder, \"dataset/arcface/Replay_Attack_training_array.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "# === Helper Functions ===\n",
    "def get_folders(folder_types=[\"train\"], image_type=\"attack\"):\n",
    "    \"\"\"\n",
    "    Retrieve .mov file paths for Replay-Attack dataset based on folder types and image type.\n",
    "\n",
    "    Args:\n",
    "        folder_types (list): List of folder types (e.g., [\"train\", \"devel\", \"test\"]).\n",
    "        image_type (str): Type of images (\"attack\" or \"real\").\n",
    "\n",
    "    Returns:\n",
    "        list: List of .mov file paths.\n",
    "    \"\"\"\n",
    "    final_files = []\n",
    "\n",
    "    for folder_type in folder_types:\n",
    "        init_folders = []\n",
    "        new_root = os.path.join(Replay_Attack_extracted, folder_type, image_type)\n",
    "\n",
    "        if not os.path.exists(new_root):\n",
    "            print(f\"Warning: Directory not found: {new_root}\")\n",
    "            continue\n",
    "\n",
    "        if image_type == \"attack\":\n",
    "            init_folders.append(os.path.join(new_root, \"fixed\"))\n",
    "            init_folders.append(os.path.join(new_root, \"hand\"))\n",
    "        else:\n",
    "            init_folders.append(new_root)\n",
    "\n",
    "        for folder in init_folders:\n",
    "            if not os.path.exists(folder):\n",
    "                print(f\"Warning: Directory not found: {folder}\")\n",
    "                continue\n",
    "            # Collect .mov files directly in the folder\n",
    "            files = [\n",
    "                f.path\n",
    "                for f in os.scandir(folder)\n",
    "                if f.is_file() and f.name.endswith(\".mov\")\n",
    "            ]\n",
    "            final_files.extend(files)\n",
    "            if not files:\n",
    "                print(f\"No .mov files found in: {folder}\")\n",
    "\n",
    "    return final_files\n",
    "\n",
    "\n",
    "# === Create Output Directories ===\n",
    "output_directories = [\n",
    "    output_attack_training_folder,\n",
    "    output_attack_validation_folder,\n",
    "    output_bonifade_training_folder,\n",
    "    output_bonifade_validation_folder,\n",
    "    save_augmented_images,\n",
    "]\n",
    "\n",
    "for output_dir in output_directories:\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created directory: {output_dir}\")\n",
    "\n",
    "# === Validate Caffe Model Files ===\n",
    "assert os.path.exists(protoPath), f\"Prototxt not found at: {protoPath}\"\n",
    "assert os.path.exists(modelPath), f\"Caffe model not found at: {modelPath}\"\n",
    "print(\"Caffe model files validated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd43f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# === Retrieve Training and Validation Files ===\n",
    "# Get attack training and validation files\n",
    "attack_folders_training = get_folders([\"train\", \"devel\"], \"attack\")\n",
    "attack_folders_validation = get_folders([\"test\"], \"attack\")\n",
    "\n",
    "# Get bonafide training and validation files\n",
    "bonifade_folders_training = get_folders([\"train\", \"devel\"], \"real\")\n",
    "bonifade_folders_validation = get_folders([\"test\"], \"real\")\n",
    "\n",
    "# === Organize Directories ===\n",
    "training_directories = [\n",
    "    attack_folders_training,\n",
    "    attack_folders_validation,\n",
    "    bonifade_folders_training,\n",
    "    bonifade_folders_validation,\n",
    "]\n",
    "output_directories = [\n",
    "    output_attack_training_folder,\n",
    "    output_attack_validation_folder,\n",
    "    output_bonifade_training_folder,\n",
    "    output_bonifade_validation_folder,\n",
    "]\n",
    "\n",
    "# === Create Output Directories ===\n",
    "for output_dir in output_directories:\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"Creating directory: {output_dir}\")\n",
    "        os.makedirs(output_dir)\n",
    "    else:\n",
    "        print(f\"Directory already exists: {output_dir}\")\n",
    "\n",
    "    # Create mask subdirectory for storing mask images (e.g., face detection outputs)\n",
    "    mask_dir = os.path.join(output_dir, \"mask\")\n",
    "    if not os.path.exists(mask_dir):\n",
    "        print(f\"Creating mask subdirectory: {mask_dir}\")\n",
    "        os.makedirs(mask_dir)\n",
    "    else:\n",
    "        print(f\"Mask subdirectory already exists: {mask_dir}\")\n",
    "\n",
    "# === Directory Structure Overview ===\n",
    "# The Replay_Attack/images folder will contain:\n",
    "# 1. Attack training images in output_attack_training_folder\n",
    "#    - Original attack images extracted from .mov files\n",
    "#    - Mask subdirectory for attack face detection outputs\n",
    "# 2. Attack validation images in output_attack_validation_folder\n",
    "#    - Original attack validation images extracted from .mov files\n",
    "#    - Mask subdirectory for attack validation face detection outputs\n",
    "# 3. Bonafide (real) training images in output_bonifade_training_folder\n",
    "#    - Original bonafide images extracted from .mov files\n",
    "#    - Mask subdirectory for bonafide face detection outputs\n",
    "# 4. Bonafide (real) validation images in output_bonifade_validation_folder\n",
    "#    - Original bonafide validation images extracted from .mov files\n",
    "#    - Mask subdirectory for bonafide validation face detection outputs\n",
    "\n",
    "# === Logging ===\n",
    "# Verify that files are retrieved correctly\n",
    "print(\n",
    "    f\"Attack files for training ({len(attack_folders_training)}):\",\n",
    "    attack_folders_training,\n",
    ")\n",
    "print(\n",
    "    f\"Attack files for validation ({len(attack_folders_validation)}):\",\n",
    "    attack_folders_validation,\n",
    ")\n",
    "print(\n",
    "    f\"Bonifide files for training ({len(bonifade_folders_training)}):\",\n",
    "    bonifade_folders_training,\n",
    ")\n",
    "print(\n",
    "    f\"Bonifide files for validation ({len(bonifade_folders_validation)}):\",\n",
    "    bonifade_folders_validation,\n",
    ")\n",
    "\n",
    "# Warn if any file list is empty\n",
    "if not attack_folders_training:\n",
    "    print(\n",
    "        \"Warning: No attack training files found. Check train/devel attack directories.\"\n",
    "    )\n",
    "if not attack_folders_validation:\n",
    "    print(\"Warning: No attack validation files found. Check test attack directories.\")\n",
    "if not bonifade_folders_training:\n",
    "    print(\n",
    "        \"Warning: No bonifide training files found. Check train/devel real directories.\"\n",
    "    )\n",
    "if not bonifade_folders_validation:\n",
    "    print(\"Warning: No bonifide validation files found. Check test real directories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efaff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# === Hyperparameters for Training ===\n",
    "final_x = 128\n",
    "final_y = 128\n",
    "n_channels = 3\n",
    "\n",
    "# Training parameters\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "init_learning_rate = 1e-4\n",
    "# Note: For ArcFace, the learning rate may need tuning (e.g., 1e-4) due to sensitivity to margin and scaling.\n",
    "\n",
    "# Target dimensions for resizing\n",
    "dim = (final_x, final_y)\n",
    "\n",
    "\n",
    "# === Preprocessing Function ===\n",
    "def resize_normalize_image(image, dim=dim, value=255.0):\n",
    "    \"\"\"\n",
    "    Resize and normalize an image for model input.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image (H, W, C) in BGR format (from cv2).\n",
    "        dim (tuple): Target dimensions (width, height) for resizing.\n",
    "        value (float): Value to divide pixel values by for normalization (default: 255.0 for range [0, 1]).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Resized and normalized image in range [0, 1].\n",
    "\n",
    "    Notes:\n",
    "        - Designed for Replay-Attack .mov video frames extracted as RGB images.\n",
    "        - Image size (32x32) is adopted from the original Replay-Attack pipeline but may be adjusted (e.g., to 128x128)\n",
    "          for better feature extraction with UNet-based models.\n",
    "        - Additional normalization (e.g., mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5) to scale to [-1, 1])\n",
    "          will be applied in the dataloader using torchvision.transforms.Normalize.\n",
    "    \"\"\"\n",
    "    # Validate input image\n",
    "    if image is None or image.size == 0:\n",
    "        raise ValueError(\"Input image is empty or None\")\n",
    "    if len(image.shape) != 3 or image.shape[2] != 3:\n",
    "        raise ValueError(f\"Expected 3-channel image, got shape {image.shape}\")\n",
    "\n",
    "    # Resize image to target dimensions\n",
    "    image = cv2.resize(image, dim)\n",
    "\n",
    "    # Normalize to [0, 1] by dividing by value\n",
    "    normalized_image = image / value\n",
    "\n",
    "    # Ensure values are in [0, 1]\n",
    "    normalized_image = np.clip(normalized_image, 0.0, 1.0)\n",
    "\n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92463c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# === Load Face Detection Model ===\n",
    "net = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "\n",
    "\n",
    "# Visualization Function ===\n",
    "def im_show(image, size=(15, 15), output=None):\n",
    "    \"\"\"\n",
    "    Display an image using Matplotlib and optionally save it.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Image to display (H, W, C), expected in RGB format.\n",
    "        size (tuple): Figure size (width, height) in inches.\n",
    "        output (str, optional): Path to save the image (in BGR format for cv2).\n",
    "    \"\"\"\n",
    "    # Validate image\n",
    "    if image is None or image.size == 0:\n",
    "        raise ValueError(\"Input image is empty or None\")\n",
    "    if len(image.shape) != 3 or image.shape[2] != 3:\n",
    "        raise ValueError(f\"Expected 3-channel RGB image, got shape:{image.shape}\")\n",
    "\n",
    "    # Display image\n",
    "    fig = plt.figure(figsize=size)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # Save image if output path is provided\n",
    "    if output is not None:\n",
    "        image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(output, image_bgr)\n",
    "        print(f\"Saved image to: {output}\")\n",
    "\n",
    "\n",
    "# === Face Detection Function ===\n",
    "def detect_save_face(color, output_path=None, multiple_output=False):\n",
    "    \"\"\"\n",
    "    Detect faces in an image using the Caffe model and optionally save them.\n",
    "\n",
    "    Args:\n",
    "        color (numpy.ndarray): Input image (H, W, C) in BGR format (from cv2), typically a frame from Replay-Attack .mov videos.\n",
    "        output_path (str, optional): Path to save the most confident face (in BGR format).\n",
    "        multiple_output (bool): If True, detect all faces; if False, detect only the most confident face.\n",
    "\n",
    "    Returns:\n",
    "        list: List of detected faces (in BGR format).\n",
    "        list: List of bounding box coordinates [startX, startY, endX, endY].\n",
    "    \"\"\"\n",
    "    # Validate input image\n",
    "    if color is None or color.size == 0:\n",
    "        raise ValueError(\"Input image is empty or None\")\n",
    "    if len(color.shape) != 3 or color.shape[2] != 3:\n",
    "        raise ValueError(f\"Expected 3-channel BGR image, got shape {color.shape}\")\n",
    "\n",
    "    (h, w) = color.shape[:2]\n",
    "\n",
    "    # Preprocess image for face detection\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        cv2.resize(color, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0)\n",
    "    )\n",
    "\n",
    "    # Perform face detection\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    faces = []\n",
    "    coordinates = []\n",
    "\n",
    "    if detections.shape[2] == 0:\n",
    "        print(\"Warning: No faces detected in the image\")\n",
    "        return faces, coordinates\n",
    "\n",
    "    # Find the most confident face or all faces\n",
    "    max_i = np.argmax(detections[0, 0, :, 2])\n",
    "    min_range = 0 if multiple_output else max_i\n",
    "\n",
    "    max_range = detections.shape[2] if multiple_output else max_i + 1\n",
    "\n",
    "    for i in range(min_range, max_range):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence >= face_confidence:\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # Ensure bounding box is within image dimensions\n",
    "            startX, startY = max(0, startX), max(0, startY)\n",
    "            endX, endY = min(w, endX), min(h, endY)\n",
    "\n",
    "            # Extract face\n",
    "            face = color[startY:endY, startX:endX]\n",
    "            if face.size == 0:\n",
    "                print(\n",
    "                    f\"Warning: Empty face region at coordinates [{startX}, {startY}, {endX}, {endY}]\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            faces.append(face)\n",
    "            coordinates.append([startX, startY, endX, endY])\n",
    "\n",
    "            # Save the face if output path is provided\n",
    "            if output_path is not None:\n",
    "                cv2.imwrite(output_path, face)\n",
    "                print(f\"Saved face to: {output_path} (Confidence: {confidence:.4f})\")\n",
    "\n",
    "    print(f\"Detected {len(faces)} face(s) with confidence >= {face_confidence}\")\n",
    "    return faces, coordinates\n",
    "\n",
    "\n",
    "# === File Handling Functions ===\n",
    "def get_file_name(path):\n",
    "    \"\"\"\n",
    "    Extract the file name (without extension) from a path.\n",
    "\n",
    "    Args:\n",
    "        path (str): File path.\n",
    "\n",
    "    Returns:\n",
    "        str: File name without extension.\n",
    "    \"\"\"\n",
    "    if not path or not isinstance(path, str):\n",
    "        raise ValueError(\"Path must be a non-empty string\")\n",
    "    base = os.path.basename(path)\n",
    "    return os.path.splitext(base)[0]\n",
    "\n",
    "\n",
    "def get_file_type(path):\n",
    "    \"\"\"\n",
    "    Extract the file extension from a path.\n",
    "\n",
    "    Args:\n",
    "        path (str): File path.\n",
    "\n",
    "    Returns:\n",
    "        str: File extension (e.g., '.mov', '.jpg').\n",
    "    \"\"\"\n",
    "    if not path or not isinstance(path, str):\n",
    "        raise ValueError(\"Path must be a non-empty string\")\n",
    "    base = os.path.basename(path)\n",
    "    return os.path.splitext(base)[1]  # e.g., .mov, .jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7ddcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames from videos...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_frames_from_videos(file_paths):\n",
    "    \"\"\"\n",
    "    Extract frames from .mov videos in the Replay-Attack dataset.\n",
    "\n",
    "    Args:\n",
    "        file_paths (list): List of .mov video file paths (e.g., from train/attack/fixed, train/real).\n",
    "\n",
    "    Notes:\n",
    "        - Extracts up to 18 frames per video, evenly distributed.\n",
    "        - Saves RGB frames as JPG in an 'extracted_images' subdirectory of the input video folder\n",
    "          (e.g., train/attack/fixed/extracted_images/).\n",
    "        - Adds suffixes (_fixed, _hand, _real) to frame filenames to distinguish video types.\n",
    "        - Frames will be processed for face detection and saved in output directories\n",
    "          (e.g., output_attack_training_folder).\n",
    "        - Frames are saved as frame_<filename>_<suffix>_<index>.jpg (e.g., frame_client001_fixed_0.jpg).\n",
    "    \"\"\"\n",
    "    for video_path in file_paths:\n",
    "        # Validate video file\n",
    "        if not os.path.exists(video_path) or not video_path.endswith(\".mov\"):\n",
    "            print(f\"Error: Invalid or missing video file: {video_path}\")\n",
    "            continue\n",
    "\n",
    "        # Determine video type for suffix\n",
    "        parent_dir = os.path.dirname(video_path).lower()\n",
    "        if \"fixed\" in parent_dir:\n",
    "            suffix = \"_fixed\"\n",
    "        elif \"hand\" in parent_dir:\n",
    "            suffix = \"_hand\"\n",
    "        else:\n",
    "            suffix = \"_real\"\n",
    "\n",
    "        # Output directory is 'extracted_images' subdirectory of the input video folder\n",
    "        output_dir = os.path.join(os.path.dirname(video_path), \"extracted_images\")\n",
    "        os.makedirs(\n",
    "            output_dir, exist_ok=True\n",
    "        )  # Create extracted_images if it doesn't exist\n",
    "\n",
    "        # Open video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Failed to open video: {video_path}\")\n",
    "            cap.release()\n",
    "            continue\n",
    "\n",
    "        # Get total frames and FPS\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "        # Strategy parameters\n",
    "        target_frames = 18  # Maximum number of frames to extract per video\n",
    "\n",
    "        if total_frames > target_frames:\n",
    "            # Take frames evenly distributed across the video\n",
    "            frame_interval = total_frames // target_frames\n",
    "        else:\n",
    "            # If video is shorter, take every frame\n",
    "            frame_interval = 1\n",
    "\n",
    "        frame_count = 0\n",
    "        saved_count = 0\n",
    "        save_frame = False\n",
    "\n",
    "        video_name = get_file_name(video_path)  # Extract filename without extension\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Strategy: Evenly distributed frames\n",
    "            if frame_count % frame_interval == 0:\n",
    "                save_frame = True\n",
    "\n",
    "            # Save frame if flagged\n",
    "            if save_frame:\n",
    "                # Validate frame\n",
    "                if frame is None:\n",
    "                    print(\n",
    "                        f\"Warning: Invalid frame at index {frame_count} in {video_path}\"\n",
    "                    )\n",
    "                    frame_count += 1\n",
    "                    continue\n",
    "\n",
    "                # Save RGB frame\n",
    "                frame_path = os.path.join(\n",
    "                    output_dir, f\"frame_{video_name}{suffix}_{saved_count}.jpg\"\n",
    "                )\n",
    "                cv2.imwrite(frame_path, frame)\n",
    "                print(f\"Saved RGB frame: {frame_path} (Shape: {frame.shape})\")\n",
    "\n",
    "                saved_count += 1\n",
    "                save_frame = False\n",
    "\n",
    "                # Stop if we've reached the target number of frames\n",
    "                if saved_count >= target_frames:\n",
    "                    break\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "        cap.release()\n",
    "        print(\n",
    "            f\"Extracted {saved_count} frames from {video_path} (total frames: {total_frames}, interval: {frame_interval})\"\n",
    "        )\n",
    "\n",
    "\n",
    "# === Extract Frames from All Videos ===\n",
    "print(\"Extracting frames from videos...\")\n",
    "for file_list in training_directories:\n",
    "    print(f\"Processing {len(file_list)} videos...\")\n",
    "    extract_frames_from_videos(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9dba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "from glob import glob\n",
    "\n",
    "# Maximum number of images per folder due to hardware constraints\n",
    "max_images_per_folder = 3000\n",
    "\n",
    "\n",
    "def take_images():\n",
    "    \"\"\"\n",
    "    Process RGB images from extracted frames to detect and extract faces.\n",
    "    For each color (RGB) image:\n",
    "    - Detects and extracts the most confident face using detect_save_face (Cell 6).\n",
    "    - Saves the face image to the appropriate output directory (from Cell 3).\n",
    "    - Limits the number of images per folder to max_images_per_folder.\n",
    "\n",
    "    Notes:\n",
    "        - Images are shuffled with a fixed seed for reproducibility.\n",
    "        - Faces are saved in BGR format and will be converted to RGB and resized to 32x32 in the dataloader.\n",
    "        - Processes frames from extracted_images/ subdirectories (e.g., train/attack/fixed/extracted_images/)\n",
    "          created in Cell 7 from .mov files.\n",
    "    \"\"\"\n",
    "    directory_counter = 0  # Track which training directory we're processing\n",
    "    file_counter = 0  # Track total files processed\n",
    "\n",
    "    for main_list in training_directories:\n",
    "        folder_file_counter = 0  # Track files processed in current folder\n",
    "\n",
    "        print(\n",
    "            f\"Processing directory {directory_counter + 1}/{len(training_directories)} \"\n",
    "            f\"({output_directories[directory_counter]})\"\n",
    "        )\n",
    "\n",
    "        # Get unique extracted_images directories from video paths\n",
    "        frame_dirs = set()\n",
    "        for video_path in main_list:\n",
    "            # Derive extracted_images directory from video path\n",
    "            frames_dir = os.path.join(os.path.dirname(video_path), \"extracted_images\")\n",
    "            if os.path.exists(frames_dir):\n",
    "                frame_dirs.add(frames_dir)\n",
    "            else:\n",
    "                print(f\"Error: Frames directory not found: {frames_dir}\")\n",
    "\n",
    "        if not frame_dirs:\n",
    "            print(\n",
    "                f\"Warning: No frame directories found for directory {directory_counter + 1}\"\n",
    "            )\n",
    "            directory_counter += 1\n",
    "            continue\n",
    "\n",
    "        for frames_dir in frame_dirs:\n",
    "            # Get all image paths in current extracted_images directory\n",
    "            list_images = glob(os.path.join(frames_dir, \"*.jpg\"))\n",
    "            if not list_images:\n",
    "                print(f\"Warning: No images found in {frames_dir}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Found {len(list_images)} images in {frames_dir}\")\n",
    "\n",
    "            # Shuffle images with fixed seed for reproducibility\n",
    "            random.Random(20).shuffle(list_images)\n",
    "\n",
    "            for file_path in list_images:\n",
    "                # Stop if we've hit the per-folder limit\n",
    "                if folder_file_counter >= max_images_per_folder:\n",
    "                    print(\n",
    "                        f\"Reached max images ({max_images_per_folder}) for folder {directory_counter + 1}\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "                # Load image\n",
    "                color = cv2.imread(file_path)\n",
    "                if color is None:\n",
    "                    print(f\"Could not load image: {file_path}\")\n",
    "                    continue\n",
    "\n",
    "                # Define output path for the face, preserving suffix (_fixed, _hand, _real)\n",
    "                frame_name = get_file_name(file_path)\n",
    "                output_path = os.path.join(\n",
    "                    output_directories[directory_counter],\n",
    "                    f\"face_{frame_name}{get_file_type(file_path)}\",\n",
    "                )\n",
    "\n",
    "                # Detect and save face\n",
    "                faces, _ = detect_save_face(color, output_path)\n",
    "                if not faces:\n",
    "                    print(f\"No faces detected in {file_path}\")\n",
    "                    continue\n",
    "\n",
    "                folder_file_counter += 1\n",
    "                file_counter += 1\n",
    "                print(\n",
    "                    f\"Saved face {file_counter} from {file_path} to folder {directory_counter + 1}\"\n",
    "                )\n",
    "\n",
    "        print(\n",
    "            f\"Total faces saved in folder {directory_counter + 1}: {folder_file_counter}\"\n",
    "        )\n",
    "        directory_counter += 1\n",
    "\n",
    "\n",
    "# Run the face extraction\n",
    "take_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Constants (from Cell 5)\n",
    "dim = (128, 128)\n",
    "final_x, final_y, n_channels = 128, 128, 3\n",
    "\n",
    "\n",
    "# === Image Preprocessing ===\n",
    "def resize_image(image, dim=dim):\n",
    "    \"\"\"\n",
    "    Resize and normalize image to target dimensions using resize_normalize_image.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image (H, W, C) in BGR format.\n",
    "        dim (tuple): Target dimensions (width, height).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Resized and normalized image in [0, 1] float32, BGR format.\n",
    "    \"\"\"\n",
    "    if image is None or image.size == 0:\n",
    "        raise ValueError(\"Input image is empty or None\")\n",
    "    image = resize_normalize_image(image, dim=dim, value=255.0)\n",
    "    return image.astype(np.float32)  # Ensure float32 for OpenCV compatibility\n",
    "\n",
    "\n",
    "# === Dataset Preparation ===\n",
    "def get_images(directories, balance_dataset=True, undersampling=False):\n",
    "    \"\"\"\n",
    "    Load images from directories, assign labels, and optionally balance the dataset.\n",
    "\n",
    "    Args:\n",
    "        directories (list): List of directories containing face images (bonafide, attack).\n",
    "        balance_dataset (bool): If True, balance classes using oversampling or undersampling.\n",
    "        undersampling (bool): If True and balance_dataset=True, use undersampling instead of oversampling.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of images (N, H, W, C) in [0, 1] float32, RGB format.\n",
    "        numpy.ndarray: Array of labels (0: bonafide, 1: attack).\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    label_list = []\n",
    "    images_per_class = []\n",
    "\n",
    "    # Set seeds for reproducibility\n",
    "    torch.manual_seed(20)\n",
    "    random.seed(20)\n",
    "    np.random.seed(20)\n",
    "\n",
    "    for class_idx, directory in enumerate(directories):\n",
    "        if not os.path.exists(directory):\n",
    "            print(f\"Error: Directory not found: {directory}\")\n",
    "            continue\n",
    "\n",
    "        list_images = [\n",
    "            f\n",
    "            for f in os.listdir(directory)\n",
    "            if f.endswith(\".jpg\") and os.path.isfile(os.path.join(directory, f))\n",
    "        ]\n",
    "        if not list_images:\n",
    "            print(f\"Warning: No images found in {directory}\")\n",
    "            continue\n",
    "\n",
    "        images_number = len(list_images)\n",
    "        if balance_dataset and undersampling and images_per_class:\n",
    "            # Limit to the size of the smallest class seen so far\n",
    "            images_number = min(images_number, min(images_per_class))\n",
    "        random.Random(20).shuffle(list_images)\n",
    "        print(f\"Found {images_number} images in {directory}\")\n",
    "\n",
    "        X = np.empty((images_number, final_x, final_y, n_channels), dtype=np.float32)\n",
    "        L = np.empty(images_number, dtype=np.int64)\n",
    "\n",
    "        ipp = 0\n",
    "        for im_name in list_images:\n",
    "            if ipp >= images_number:\n",
    "                break\n",
    "            im_path = os.path.join(directory, im_name)\n",
    "            image = cv2.imread(im_path)\n",
    "            if image is None:\n",
    "                print(f\"Failed to load image: {im_path}\")\n",
    "                continue\n",
    "\n",
    "            # Resize and normalize to [0, 1], convert to RGB\n",
    "            image = resize_image(image, dim=dim)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            X[ipp] = image\n",
    "            L[ipp] = class_idx  # 0: bonafide, 1: attack\n",
    "            ipp += 1\n",
    "\n",
    "        image_list.append(X[:ipp])\n",
    "        label_list.append(L[:ipp])\n",
    "        images_per_class.append(ipp)\n",
    "\n",
    "    if not image_list:\n",
    "        raise ValueError(\"No images loaded from any directory\")\n",
    "\n",
    "    # Balance dataset\n",
    "    if balance_dataset:\n",
    "        if undersampling:\n",
    "            # Undersample majority class to match minority\n",
    "            min_class_size = min(images_per_class)\n",
    "            for j in range(len(images_per_class)):\n",
    "                if images_per_class[j] > min_class_size:\n",
    "                    indices = np.random.choice(\n",
    "                        images_per_class[j], min_class_size, replace=False\n",
    "                    )\n",
    "                    image_list[j] = image_list[j][indices]\n",
    "                    label_list[j] = label_list[j][indices]\n",
    "                    images_per_class[j] = min_class_size\n",
    "                    print(f\"Undersampled class {j} to {min_class_size} images\")\n",
    "        else:\n",
    "            # Oversample minority class to match majority\n",
    "            max_class_size = max(images_per_class)\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.RandomRotation(10),\n",
    "                    transforms.RandomHorizontalFlip(p=0.3),\n",
    "                    transforms.ColorJitter(\n",
    "                        brightness=0.1, contrast=0.1, saturation=0.1\n",
    "                    ),\n",
    "                    transforms.ToTensor(),  # [C, H, W], [0, 1]\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            for j in range(len(images_per_class)):\n",
    "                if images_per_class[j] < max_class_size:\n",
    "                    diff = max_class_size - images_per_class[j]\n",
    "                    image_array = image_list[j]\n",
    "                    label_array = label_list[j]\n",
    "\n",
    "                    new_images = np.empty(\n",
    "                        (diff, final_x, final_y, n_channels), dtype=np.float32\n",
    "                    )\n",
    "                    new_labels = np.full(diff, j, dtype=np.int64)\n",
    "\n",
    "                    for k in range(diff):\n",
    "                        idx = random.randint(0, len(image_array) - 1)\n",
    "                        img = image_array[idx]  # [H, W, C], [0, 1], RGB\n",
    "                        img = (img * 255).astype(\n",
    "                            np.uint8\n",
    "                        )  # [0, 255] uint8 for ToPILImage\n",
    "                        aug_img = (\n",
    "                            transform(img).permute(1, 2, 0).numpy()\n",
    "                        )  # [H, W, C], [0, 1], RGB\n",
    "                        new_images[k] = aug_img\n",
    "\n",
    "                    image_list[j] = np.concatenate([image_array, new_images], axis=0)\n",
    "                    label_list[j] = np.concatenate([label_array, new_labels], axis=0)\n",
    "                    images_per_class[j] = max_class_size\n",
    "                    print(f\"Oversampled class {j} by {diff} images\")\n",
    "\n",
    "    # Combine classes\n",
    "    new_image_list = np.concatenate(image_list, axis=0)\n",
    "    new_label_list = np.concatenate(label_list, axis=0)\n",
    "\n",
    "    # Verify class balance\n",
    "    label_counts = Counter(new_label_list)\n",
    "    print(f\"Final dataset: {label_counts}\")\n",
    "    if label_counts[0] != label_counts[1] and balance_dataset:\n",
    "        print(\"Warning: Classes are not balanced despite balancing attempt!\")\n",
    "\n",
    "    return new_image_list, new_label_list\n",
    "\n",
    "\n",
    "# === Load or Generate Data ===\n",
    "read_from_np_array = False\n",
    "print(f\"Reading from numpy array: {read_from_np_array}\")\n",
    "if read_from_np_array:\n",
    "    with open(output_np_training_array, \"rb\") as f:\n",
    "        train_X, test_X, train_Y, test_Y = pickle.load(f)\n",
    "else:\n",
    "    # Load training data\n",
    "    train_X, train_Y = get_images(\n",
    "        [output_bonifade_training_folder, output_attack_training_folder],\n",
    "        balance_dataset=True,\n",
    "        undersampling=False,  # Default to oversampling\n",
    "    )\n",
    "\n",
    "    # Load validation data\n",
    "    test_X, test_Y = get_images(\n",
    "        [output_bonifade_validation_folder, output_attack_validation_folder],\n",
    "        balance_dataset=False,\n",
    "    )\n",
    "\n",
    "    # Save data to disk\n",
    "    os.makedirs(os.path.dirname(save_labels), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(output_np_training_array), exist_ok=True)\n",
    "\n",
    "    # with open(output_np_training_array, \"wb\") as f:\n",
    "    #     pickle.dump((train_X, test_X, train_Y, test_Y), f)\n",
    "    # print(f\"Saved dataset to {output_np_training_array}\")\n",
    "\n",
    "print(\"Shape training:\", train_X.shape)\n",
    "print(\"Shape validation:\", test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd4dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Settings\n",
    "augmentation_online = True  # Online augmentation for training\n",
    "generator_seed = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Define transformations\n",
    "base_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Custom Dataset class\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (numpy.ndarray): [N, H, W, C], [0, 1], RGB format (32x32 for Replay-Attack).\n",
    "            labels (numpy.ndarray): [N], int64, 0 (bonafide) or 1 (attack).\n",
    "            transform: Optional transform to apply.\n",
    "        \"\"\"\n",
    "        self.images = images  # [N, H, W, C], [0, 1], RGB\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]  # [H, W, C], [0, 1], RGB\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # [C, H, W], [-1, 1]\n",
    "        else:\n",
    "            image = (\n",
    "                torch.from_numpy(image).permute(2, 0, 1).float()\n",
    "            )  # [C, H, W], [0, 1]\n",
    "            image = (image - 0.5) / 0.5  # Normalize to [-1, 1]\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.long)  # For nn.CrossEntropyLoss\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Create generator with seed for reproducibility\n",
    "g = torch.Generator()\n",
    "g.manual_seed(generator_seed)\n",
    "\n",
    "# Datasets and loaders\n",
    "train_dataset = CustomImageDataset(\n",
    "    train_X,\n",
    "    train_Y,\n",
    "    transform=base_transforms,\n",
    ")\n",
    "val_dataset = CustomImageDataset(test_X, test_Y, transform=base_transforms)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    generator=g,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# Check lengths\n",
    "print(\"Length of training batches:\", len(train_loader))\n",
    "print(\"Total training samples:\", len(train_X))\n",
    "print(\"Total validation samples:\", len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c9222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# Settings\n",
    "augmentation_online = False  # Offline augmentation enabled\n",
    "generator_seed = 10\n",
    "torch.manual_seed(generator_seed)\n",
    "np.random.seed(generator_seed)\n",
    "\n",
    "# Constants (from Cell 5)\n",
    "final_x, final_y, n_channels = 128, 128, 3\n",
    "\n",
    "# Define offline augmentation transform (moderate)\n",
    "offline_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomAffine(\n",
    "            degrees=15,\n",
    "            scale=(0.85, 1.15),\n",
    "        ),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),  # [C, H, W], [0, 1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Function to augment dataset\n",
    "def augment_dataset(X, Y, multiplier=2):\n",
    "    \"\"\"\n",
    "    Augment the dataset by creating multiple augmented versions of each image.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): Images, [N, 128, 128, 3], [0, 1], RGB.\n",
    "        Y (numpy.ndarray): Labels, [N], int64, 0 (bonafide) or 1 (attack).\n",
    "        multiplier (int): Number of copies (original + augmented) per image.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Augmented images, [N*multiplier, 128, 128, 3], [0, 1], RGB, float32.\n",
    "        numpy.ndarray: Augmented labels, [N*multiplier], int64.\n",
    "    \"\"\"\n",
    "    if len(X) != len(Y):\n",
    "        raise ValueError(f\"Images ({len(X)}) and labels ({len(Y)}) length mismatch\")\n",
    "\n",
    "    print(f\"Original dataset: {Counter(Y)}\")\n",
    "    to_augment = len(X) * multiplier\n",
    "    new_X = np.empty((to_augment, final_x, final_y, n_channels), dtype=np.float32)\n",
    "    new_Y = np.empty((to_augment), dtype=np.int64)\n",
    "\n",
    "    idx = 0\n",
    "    for i in tqdm(range(len(X)), desc=\"Augmenting\"):\n",
    "        img = X[i]  # [32, 32, 3], [0, 1], RGB\n",
    "        label = Y[i]\n",
    "\n",
    "        # Store original\n",
    "        new_X[idx] = img\n",
    "        new_Y[idx] = label\n",
    "        idx += 1\n",
    "\n",
    "        # Augment (multiplier-1) times\n",
    "        for _ in range(multiplier - 1):\n",
    "            img_uint8 = (img * 255.0).astype(np.uint8)  # [0, 255] for ToPILImage\n",
    "            aug_img_tensor = offline_transform(img_uint8)  # [C, H, W], [0, 1]\n",
    "            aug_img_np = aug_img_tensor.permute(1, 2, 0).numpy()  # [32, 32, 3], [0, 1]\n",
    "            new_X[idx] = aug_img_np\n",
    "            new_Y[idx] = label\n",
    "            idx += 1\n",
    "\n",
    "    print(f\"Augmented dataset: {Counter(new_Y)}\")\n",
    "    if Counter(new_Y)[0] != Counter(new_Y)[1]:\n",
    "        print(\"Warning: Augmented dataset is not balanced!\")\n",
    "    return new_X, new_Y\n",
    "\n",
    "\n",
    "# Apply augmentation\n",
    "if not augmentation_online:\n",
    "    train_X, train_Y = augment_dataset(train_X, train_Y, multiplier=2)\n",
    "\n",
    "print(\"Shape training:\", train_X.shape)\n",
    "print(\"Shape validation:\", test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1455da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets and loaders\n",
    "train_dataset = CustomImageDataset(train_X, train_Y, transform=base_transforms)\n",
    "val_dataset = CustomImageDataset(test_X, test_Y, transform=base_transforms)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    generator=g,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# Check lengths\n",
    "print(\"Length of training batches:\", len(train_loader))\n",
    "print(\"Total training samples:\", len(train_X))\n",
    "print(\"Total validation samples:\", len(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ff723",
   "metadata": {},
   "source": [
    "# Training (Unet architecture (CELoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd92c566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import psutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Define MemoryUsage class for tracking memory usage\n",
    "class MemoryUsage:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.max_RAM = []\n",
    "        self.max_GPU = []\n",
    "\n",
    "    def get_size(self, byte):\n",
    "        factor = 1024\n",
    "        for unit in [\"\", \"K\", \"M\", \"GB\", \"T\", \"P\"]:\n",
    "            if byte < factor:\n",
    "                return f\"{byte:.2f} {unit}B\"\n",
    "            byte /= factor\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        svmem = psutil.virtual_memory()\n",
    "        self.max_RAM.append((svmem.used, svmem.total, svmem.percent))\n",
    "        if torch.cuda.is_available():\n",
    "            self.max_GPU.append(torch.cuda.memory_allocated())  # Store in bytes\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    def on_train_end(self):\n",
    "        i = np.argmax([r[0] for r in self.max_RAM])\n",
    "        print(\n",
    "            \"MAX RAM USAGE: %s / %s (%s%%)\"\n",
    "            % (\n",
    "                self.get_size(self.max_RAM[i][0]),\n",
    "                self.get_size(self.max_RAM[i][1]),\n",
    "                self.max_RAM[i][2],\n",
    "            )\n",
    "        )\n",
    "        if self.max_GPU:\n",
    "            print(\"MAX GPU USAGE: %s\" % self.get_size(max(self.max_GPU)))\n",
    "\n",
    "\n",
    "# Compute class-wise accuracy for binary classification (two-class output)\n",
    "def compute_class_accuracy(outputs, targets):\n",
    "    \"\"\"\n",
    "    Compute class-wise accuracy for binary classification.\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): [batch_size, 2], raw logits.\n",
    "        targets (torch.Tensor): [batch_size], long, 0 or 1.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Correct predictions per class.\n",
    "        torch.Tensor: Total samples per class.\n",
    "    \"\"\"\n",
    "    predicted = torch.argmax(outputs, dim=1)  # [batch_size], predicted class indices\n",
    "    correct_per_class = torch.zeros(2, device=targets.device)\n",
    "    total_per_class = torch.zeros(2, device=targets.device)\n",
    "    for i in range(2):\n",
    "        mask = targets == i\n",
    "        correct_per_class[i] = (predicted[mask] == targets[mask]).sum().float()\n",
    "        total_per_class[i] = mask.sum().float()\n",
    "    return correct_per_class, total_per_class\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    criterion,\n",
    "    device,\n",
    "    num_epochs=50,\n",
    "    early_stop_patience=3,\n",
    "    save_dir=\"./models\",\n",
    "    best_model_pth=\"best_model.pth\",\n",
    "    last_model_pth=\"last_model.pth\",\n",
    "    save_training_metrics_plot=\"training_plot.png\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the UNetBinaryClassifier_CE model with CrossEntropyLoss.\n",
    "\n",
    "    Args:\n",
    "        model: UNetBinaryClassifier_CE model.\n",
    "        train_loader: DataLoader for training data.\n",
    "        val_loader: DataLoader for validation data.\n",
    "        optimizer: Optimizer (e.g., Adam).\n",
    "        scheduler: Learning rate scheduler (e.g., ReduceLROnPlateau).\n",
    "        criterion: Loss function (e.g., CrossEntropyLoss).\n",
    "        device: Device to train on (cuda or cpu).\n",
    "        num_epochs (int): Number of epochs to train.\n",
    "        early_stop_patience (int): Patience for early stopping.\n",
    "        save_dir (str): Directory to save models and plots.\n",
    "        best_model_pth (str): Filename for best model checkpoint.\n",
    "        last_model_pth (str): Filename for last model checkpoint.\n",
    "        save_training_metrics_plot (str): Filename for training metrics plot.\n",
    "\n",
    "    Returns:\n",
    "        dict: Training history with metrics.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"train_f1\": [],\n",
    "        \"val_f1\": [],\n",
    "        \"train_precision\": [],\n",
    "        \"val_precision\": [],\n",
    "        \"train_recall\": [],\n",
    "        \"val_recall\": [],\n",
    "        \"train_class_acc\": [],\n",
    "        \"val_class_acc\": [],\n",
    "        \"train_confusion_matrix\": [],\n",
    "        \"val_confusion_matrix\": [],\n",
    "    }\n",
    "    best_val_loss = float(\"inf\")\n",
    "    early_stop_counter = 0\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    best_model_pth = os.path.join(save_dir, best_model_pth)\n",
    "    last_model_pth = os.path.join(save_dir, last_model_pth)\n",
    "    memory_tracker = MemoryUsage()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        y_true, y_pred = [], []\n",
    "        train_correct_per_class = torch.zeros(2, device=device)\n",
    "        train_total_per_class = torch.zeros(2, device=device)\n",
    "        all_logits = []\n",
    "        all_probs = []\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(\n",
    "            tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "        ):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.view(-1).long()  # [batch_size], long, 0 or 1\n",
    "            if not torch.all((labels == 0) | (labels == 1)):\n",
    "                print(f\"Invalid targets: {torch.unique(labels)}\")\n",
    "                break\n",
    "            if torch.isnan(inputs).any() or torch.isinf(inputs).any():\n",
    "                print(f\"Invalid inputs at epoch {epoch}, batch {batch_idx}\")\n",
    "                break\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # [batch_size, 2], raw logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"Invalid loss at epoch {epoch}, batch {batch_idx}\")\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)  # [batch_size, 2], probabilities\n",
    "            predicted = torch.argmax(\n",
    "                outputs, dim=1\n",
    "            )  # [batch_size], predicted class indices\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            correct_per_class, total_per_class = compute_class_accuracy(outputs, labels)\n",
    "            train_correct_per_class += correct_per_class\n",
    "            train_total_per_class += total_per_class\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            all_logits.extend(outputs.cpu().detach().numpy())\n",
    "            all_probs.extend(probs.cpu().detach().numpy())\n",
    "\n",
    "        # Log distribution of logits and probabilities\n",
    "        all_logits = np.array(all_logits)  # [num_samples, 2]\n",
    "        all_probs = np.array(all_probs)  # [num_samples, 2]\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} - Train Logits (Class 0): Min: {all_logits[:, 0].min():.4f}, Max: {all_logits[:, 0].max():.4f}, Mean: {all_logits[:, 0].mean():.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} - Train Logits (Class 1): Min: {all_logits[:, 1].min():.4f}, Max: {all_logits[:, 1].max():.4f}, Mean: {all_logits[:, 1].mean():.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} - Train Probabilities (Class 0): Min: {all_probs[:, 0].min():.4f}, Max: {all_probs[:, 0].max():.4f}, Mean: {all_probs[:, 0].mean():.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} - Train Probabilities (Class 1): Min: {all_probs[:, 1].min():.4f}, Max: {all_probs[:, 1].max():.4f}, Mean: {all_probs[:, 1].mean():.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} - Predicted Class Distribution: Class 0: {np.sum(np.array(y_pred) == 0)}, Class 1: {np.sum(np.array(y_pred) == 1)}\"\n",
    "        )\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = correct / total\n",
    "        epoch_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        precision, recall, _, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=\"macro\", zero_division=0\n",
    "        )\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        train_class_acc = (\n",
    "            100.0 * train_correct_per_class / (train_total_per_class + 1e-8)\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        val_y_true, val_y_pred = [], []\n",
    "        val_correct_per_class = torch.zeros(2, device=device)\n",
    "        val_total_per_class = torch.zeros(2, device=device)\n",
    "        val_all_logits = []\n",
    "        val_all_probs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in tqdm(\n",
    "                val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"\n",
    "            ):\n",
    "                val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                val_labels = val_labels.view(-1).long()\n",
    "                outputs = model(val_inputs)  # [batch_size, 2], raw logits\n",
    "                v_loss = criterion(outputs, val_labels)\n",
    "                v_probs = torch.softmax(outputs, dim=1)\n",
    "                v_predicted = torch.argmax(outputs, dim=1)\n",
    "                val_loss += v_loss.item() * val_inputs.size(0)\n",
    "                correct_per_class, total_per_class = compute_class_accuracy(\n",
    "                    outputs, val_labels\n",
    "                )\n",
    "                val_correct_per_class += correct_per_class\n",
    "                val_total_per_class += total_per_class\n",
    "                val_correct += (v_predicted == val_labels).sum().item()\n",
    "                val_total += val_labels.size(0)\n",
    "                val_y_true.extend(val_labels.cpu().numpy())\n",
    "                val_y_pred.extend(v_predicted.cpu().numpy())\n",
    "                val_all_logits.extend(outputs.cpu().numpy())\n",
    "                val_all_probs.extend(v_probs.cpu().numpy())\n",
    "\n",
    "        # Log validation distribution\n",
    "        val_all_logits = np.array(val_all_logits)  # [num_samples, 2]\n",
    "        val_all_probs = np.array(val_all_probs)  # [num_samples, 2]\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} - Val Logits (Class 0): Min: {val_all_logits[:, 0].min():.4f}, Max: {val_all_logits[:, 0].max():.4f}, Mean: {val_all_logits[:, 0].mean():.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} - Val Logits (Class 1): Min: {val_all_logits[:, 1].min():.4f}, Max: {val_all_logits[:, 1].max():.4f}, Mean: {val_all_logits[:, 1].mean():.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} - Val Probabilities (Class 0): Min: {val_all_probs[:, 0].min():.4f}, Max: {val_all_probs[:, 0].max():.4f}, Mean: {val_all_probs[:, 0].mean():.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} - Val Probabilities (Class 1): Min: {val_all_probs[:, 1].min():.4f}, Max: {val_all_probs[:, 1].max():.4f}, Mean: {val_all_probs[:, 1].mean():.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Epoch {epoch+1} - Val Predicted Class Distribution: Class 0: {np.sum(np.array(val_y_pred) == 0)}, Class 1: {np.sum(np.array(val_y_pred) == 1)}\"\n",
    "        )\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        val_f1 = f1_score(\n",
    "            val_y_true, val_y_pred, average=\"weighted\"\n",
    "        )  # Weighted due to imbalance\n",
    "        val_precision, val_recall, _, _ = precision_recall_fscore_support(\n",
    "            val_y_true, val_y_pred, average=\"weighted\", zero_division=0\n",
    "        )\n",
    "        val_cm = confusion_matrix(val_y_true, val_y_pred)\n",
    "        val_class_acc = 100.0 * val_correct_per_class / (val_total_per_class + 1e-8)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
    "            f\"F1: {epoch_f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Train Class-wise Acc: Class 0: {train_class_acc[0]:.2f}%, Class 1: {train_class_acc[1]:.2f}%\"\n",
    "        )\n",
    "        print(f\"Train Confusion Matrix:\\n{cm}\")\n",
    "        print(\n",
    "            f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, \"\n",
    "            f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Val Class-wise Acc: Class 0: {val_class_acc[0]:.2f}%, Class 1: {val_class_acc[1]:.2f}%\"\n",
    "        )\n",
    "        print(f\"Val Confusion Matrix:\\n{val_cm}\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        history[\"train_loss\"].append(epoch_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(epoch_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"train_f1\"].append(epoch_f1)\n",
    "        history[\"val_f1\"].append(val_f1)\n",
    "        history[\"train_precision\"].append(precision)\n",
    "        history[\"val_precision\"].append(val_precision)\n",
    "        history[\"train_recall\"].append(recall)\n",
    "        history[\"val_recall\"].append(val_recall)\n",
    "        history[\"train_class_acc\"].append(train_class_acc.tolist())\n",
    "        history[\"val_class_acc\"].append(val_class_acc.tolist())\n",
    "        history[\"train_confusion_matrix\"].append(cm.tolist())\n",
    "        history[\"val_confusion_matrix\"].append(val_cm.tolist())\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        memory_tracker.on_epoch_end()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": (\n",
    "                        scheduler.state_dict() if scheduler else None\n",
    "                    ),\n",
    "                    \"train_loss\": epoch_loss,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"train_acc\": epoch_acc,\n",
    "                    \"val_acc\": val_acc,\n",
    "                    \"train_f1\": epoch_f1,\n",
    "                    \"val_f1\": val_f1,\n",
    "                    \"train_precision\": precision,\n",
    "                    \"val_precision\": val_precision,\n",
    "                    \"train_recall\": recall,\n",
    "                    \"val_recall\": val_recall,\n",
    "                    \"train_class_acc\": train_class_acc.tolist(),\n",
    "                    \"val_class_acc\": val_class_acc.tolist(),\n",
    "                },\n",
    "                best_model_pth,\n",
    "            )\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n",
    "            \"train_loss\": epoch_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"train_acc\": epoch_acc,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"train_f1\": epoch_f1,\n",
    "            \"val_f1\": val_f1,\n",
    "            \"train_precision\": precision,\n",
    "            \"val_precision\": val_precision,\n",
    "            \"train_recall\": recall,\n",
    "            \"val_recall\": val_recall,\n",
    "            \"train_class_acc\": train_class_acc.tolist(),\n",
    "            \"val_class_acc\": val_class_acc.tolist(),\n",
    "        },\n",
    "        last_model_pth,\n",
    "    )\n",
    "\n",
    "    memory_tracker.on_train_end()\n",
    "\n",
    "    def plot_metric_subplot(index, metrics, title, ylabel, is_classwise=False):\n",
    "        plt.subplot(4, 1, index)\n",
    "\n",
    "        if is_classwise:\n",
    "            for cls_idx in range(2):  # Assuming binary classification: Class 0 and 1\n",
    "                plt.plot(\n",
    "                    [x[cls_idx] for x in history[f\"train_{metrics}\"]],\n",
    "                    label=f\"Train Class {cls_idx} Acc\",\n",
    "                )\n",
    "                plt.plot(\n",
    "                    [x[cls_idx] for x in history[f\"val_{metrics}\"]],\n",
    "                    label=f\"Val Class {cls_idx} Acc\",\n",
    "                )\n",
    "        else:\n",
    "            for phase in [\"train\", \"val\"]:\n",
    "                for metric in metrics:\n",
    "                    plt.plot(\n",
    "                        history[f\"{phase}_{metric}\"],\n",
    "                        label=f\"{phase.capitalize()} {metric.capitalize()}\",\n",
    "                    )\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "\n",
    "    # === Plotting ===\n",
    "    plt.figure(figsize=(12, 16))\n",
    "\n",
    "    plot_metric_subplot(1, [\"loss\"], \"Training and Validation Loss\", \"Loss\")\n",
    "    plot_metric_subplot(2, [\"acc\"], \"Training and Validation Accuracy\", \"Accuracy\")\n",
    "    plot_metric_subplot(\n",
    "        3, [\"precision\", \"recall\"], \"Training and Validation Precision/Recall\", \"Score\"\n",
    "    )\n",
    "    plot_metric_subplot(\n",
    "        4, \"class_acc\", \"Class-wise Accuracy\", \"Accuracy (%)\", is_classwise=True\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, save_training_metrics_plot))\n",
    "    plt.close()\n",
    "\n",
    "    np.save(os.path.join(save_dir, \"training_metrics.npy\"), history)\n",
    "    return history\n",
    "\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Function to get learning rate for logging\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "\n",
    "# Inspect data distribution\n",
    "def inspect_dataloader(loader, name=\"DataLoader\"):\n",
    "    labels = []\n",
    "    for _, lbls in loader:\n",
    "        labels.extend(lbls.cpu().numpy())\n",
    "    label_counts = Counter(labels)\n",
    "    total = len(labels)\n",
    "    print(f\"{name} Class Distribution:\")\n",
    "    print(f\"Class 0: {label_counts[0]} ({100 * label_counts[0] / total:.2f}%)\")\n",
    "    print(f\"Class 1: {label_counts[1]} ({100 * label_counts[1] / total:.2f}%)\")\n",
    "    return label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db17ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Function to get learning rate for logging\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "\n",
    "# Inspect data distribution\n",
    "def inspect_dataloader(loader, name=\"DataLoader\"):\n",
    "    labels = []\n",
    "    for _, lbls in loader:\n",
    "        labels.extend(lbls.cpu().numpy())\n",
    "    label_counts = Counter(labels)\n",
    "    total = len(labels)\n",
    "    print(f\"{name} Class Distribution:\")\n",
    "    print(f\"Class 0: {label_counts[0]} ({100 * label_counts[0] / total:.2f}%)\")\n",
    "    print(f\"Class 1: {label_counts[1]} ({100 * label_counts[1] / total:.2f}%)\")\n",
    "    return label_counts\n",
    "\n",
    "\n",
    "# Inspect train and validation loaders\n",
    "print(\"Inspecting Data Loaders...\")\n",
    "train_label_counts = inspect_dataloader(train_loader, \"Training\")\n",
    "val_label_counts = inspect_dataloader(val_loader, \"Validation\")\n",
    "\n",
    "\n",
    "# Apply weight initialization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "# Initialize model and training components\n",
    "model = UNetBinaryClassifier_CE(in_channels=3, base_channels=32).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()  #  # For binary classification with two-class output\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=3\n",
    ")\n",
    "\n",
    "# Verify input normalization\n",
    "sample_batch, sample_labels = next(iter(train_loader))\n",
    "print(\"Input range:\", sample_batch.min().item(), \"to\", sample_batch.max().item())\n",
    "if sample_batch.max() > 1.0 or sample_batch.min() < -1.0:\n",
    "    print(\"Warning: Inputs are not normalized to [-1, 1].\")\n",
    "\n",
    "# Verify model output shape for a batch\n",
    "sample_batch = torch.randn(32, 3, 128, 128).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(sample_batch)\n",
    "    print(\"Model output shape for batch:\", outputs.shape)  # Expected: [32, 2]\n",
    "\n",
    "# Train the model with custom save paths\n",
    "history = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    num_epochs=50,\n",
    "    early_stop_patience=3,\n",
    "    save_dir=\"model_save_path\",\n",
    "    best_model_pth=\"model_best_checkpoint.pth\",\n",
    "    last_model_pth=\"model_last_checkpoint.pth\",\n",
    "    save_training_metrics_plot=\"training_metrics.png\",\n",
    ")\n",
    "\n",
    "# Visualize sample images to check for data leakage\n",
    "sample_batch, sample_labels = next(iter(val_loader))\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    img = sample_batch[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5  # Denormalize\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Label: {sample_labels[i].item()}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.savefig(os.path.join(\"figure_save_path\", \"sample_validation_images.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fcb57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import brentq\n",
    "import itertools\n",
    "import os\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "log_dir = \"log_path\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, f\"eval_{int(time())}.log\")\n",
    "logging.basicConfig(\n",
    "    filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(message)s\"\n",
    ")\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "# Load the saved model\n",
    "checkpoint_path = \"model_save_path/model_best_checkpoint.pth\"\n",
    "try:\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"Checkpoint file not found at {checkpoint_path}\")\n",
    "    raise\n",
    "\n",
    "# Initialize model\n",
    "model = UNetBinaryClassifier_CE(in_channels=3, base_channels=32).to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    logging.info(f\"Loaded checkpoint from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "    val_acc = checkpoint.get(\"val_acc\", \"unknown\")\n",
    "    if isinstance(val_acc, (int, float)):\n",
    "        logging.info(f\"Validation accuracy: {val_acc:.3f}%\")\n",
    "    else:\n",
    "        logging.info(f\"Validation accuracy: {val_acc}\")\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)\n",
    "    logging.info(\"Loaded model state dictionary\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Collect predictions and true labels for evaluation\n",
    "y_score = []\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, labels) in enumerate(val_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.view(-1).long()\n",
    "\n",
    "        # Validate inputs and labels\n",
    "        if inputs.shape[1] != 3 or inputs.shape[2:] != (128, 128):\n",
    "            logging.error(f\"Invalid input shape at batch {batch_idx}: {inputs.shape}\")\n",
    "            break\n",
    "        if not torch.all((labels == 0) | (labels == 1)):\n",
    "            logging.error(\n",
    "                f\"Invalid labels at batch {batch_idx}: {torch.unique(labels)}\"\n",
    "            )\n",
    "            break\n",
    "        if torch.isnan(inputs).any() or torch.isinf(inputs).any():\n",
    "            logging.error(f\"NaN/Inf in inputs at batch {batch_idx}\")\n",
    "            break\n",
    "\n",
    "        # Forward pass (model returns raw logits)\n",
    "        outputs = model(inputs)  # Shape: [batch_size, 2], raw logits\n",
    "        probs = torch.softmax(outputs, dim=1)  # Shape: [batch_size, 2], probabilities\n",
    "        preds = torch.argmax(\n",
    "            outputs, dim=1\n",
    "        )  # Shape: [batch_size], predicted class indices\n",
    "\n",
    "        if torch.isnan(probs).any() or torch.isinf(probs).any():\n",
    "            logging.error(f\"NaN/Inf in outputs at batch {batch_idx}\")\n",
    "            break\n",
    "\n",
    "        y_score.extend(probs[:, 1].cpu().numpy())  # Probability of Class 1 (Attack)\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert evaluation results to numpy arrays\n",
    "y_score = np.array(y_score)\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "\n",
    "# Compute EER\n",
    "def compute_eer(y_true, y_score):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = brentq(lambda x: 1.0 - x - interp1d(fpr, fnr)(x), 0.0, 1.0)\n",
    "    eer = interp1d(fpr, fnr)(eer_threshold)\n",
    "    return eer\n",
    "\n",
    "\n",
    "# Compute HTER\n",
    "def compute_hter(y_true, y_pred, y_score, threshold=0.5):\n",
    "    far = np.sum((y_pred == 1) & (y_true == 0)) / (np.sum(y_true == 0) + 1e-8)\n",
    "    frr = np.sum((y_pred == 0) & (y_true == 1)) / (np.sum(y_true == 1) + 1e-8)\n",
    "    hter = (far + frr) / 2\n",
    "    return hter, far, frr\n",
    "\n",
    "\n",
    "# Confusion Matrix Function\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title=\"Confusion Matrix\"):\n",
    "    plt.figure(figsize=(6, 6), dpi=80)\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / (cm.sum(axis=1)[:, np.newaxis] + 1e-8)\n",
    "        logging.info(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        logging.info(\"Confusion matrix, without normalization\")\n",
    "\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    fname = os.path.join(log_dir, f\"confusion_matrix_{int(time())}.png\")\n",
    "    plt.savefig(fname)\n",
    "    logging.info(f\"Saved confusion matrix to {fname}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Classification Report\n",
    "class_names = [\"Bonafide\", \"Attack\"]\n",
    "logging.info(\"\\nClassification Report:\")\n",
    "report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "logging.info(\"\\n\" + report)\n",
    "\n",
    "# Confusion Matrix\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=False)\n",
    "plot_confusion_matrix(\n",
    "    cnf_matrix, classes=class_names, normalize=True, title=\"Normalized Confusion Matrix\"\n",
    ")\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(5, 5), dpi=80)\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "fname = os.path.join(log_dir, f\"roc_curve_{int(time())}.png\")\n",
    "plt.savefig(fname)\n",
    "logging.info(f\"Saved ROC curve to {fname}\")\n",
    "plt.close()\n",
    "\n",
    "# EER and HTER\n",
    "eer = compute_eer(y_true, y_score)\n",
    "hter, far, frr = compute_hter(y_true, y_pred, y_score)\n",
    "logging.info(f\"FAR: {far:.4f}\")\n",
    "logging.info(f\"FRR: {frr:.4f}\")\n",
    "logging.info(f\"Equal Error Rate (EER): {eer:.4f}\")\n",
    "logging.info(f\"Half Total Error Rate (HTER): {hter:.4f}\")\n",
    "\n",
    "# Score Distribution\n",
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "plt.hist(y_score[y_true == 0], bins=50, alpha=0.5, label=\"Bonafide\", color=\"blue\")\n",
    "plt.hist(y_score[y_true == 1], bins=50, alpha=0.5, label=\"Attack\", color=\"red\")\n",
    "plt.xlabel(\"Prediction Score (Class 1 Probability)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Score Distribution\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "fname = os.path.join(log_dir, f\"score_distribution_{int(time())}.png\")\n",
    "plt.savefig(fname)\n",
    "logging.info(f\"Saved score distribution to {fname}\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ed947",
   "metadata": {},
   "source": [
    "# Training (Unet with AAM Loss architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6f538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nmodel = UNetBinaryClassifier()  # From code cell 1\\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=2)\\ncriterion = nn.CrossEntropyLoss()\\nhistory = train_model(\\n    model,\\n    train_loader,\\n    val_loader,\\n    optimizer,\\n    scheduler,\\n    criterion,\\n    device,\\n    num_epochs=30,\\n    early_stop_patience=3,\\n    save_dir=\"D:/.../dataset/training/Replay_Attack/images\",\\n    best_model_pth=\"best_model.pth\",\\n    last_model_pth=\"last_model.pth\",\\n    save_training_metrics_plot=\"training_plot.png\",\\n)\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import psutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "class MemoryUsage:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.max_RAM = []\n",
    "        self.max_GPU = []\n",
    "\n",
    "    def get_size(self, byte):\n",
    "        factor = 1024\n",
    "        for unit in [\"\", \"K\", \"M\", \"GB\", \"T\", \"P\"]:\n",
    "            if byte < factor:\n",
    "                return f\"{byte:.2f} {unit}B\"\n",
    "            byte /= factor\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        svmem = psutil.virtual_memory()\n",
    "        self.max_RAM.append((svmem.used, svmem.total, svmem.percent))\n",
    "        if torch.cuda.is_available():\n",
    "            self.max_GPU.append(torch.cuda.memory_allocated())\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    def on_train_end(self):\n",
    "        i = np.argmax([r[0] for r in self.max_RAM])\n",
    "        print(\n",
    "            \"MAX RAM USAGE: %s / %s (%s%%)\"\n",
    "            % (\n",
    "                self.get_size(self.max_RAM[i][0]),\n",
    "                self.get_size(self.max_RAM[i][1]),\n",
    "                self.max_RAM[i][2],\n",
    "            )\n",
    "        )\n",
    "        if self.max_GPU:\n",
    "            print(\"MAX GPU USAGE: %s\" % self.get_size(max(self.max_GPU)))\n",
    "\n",
    "\n",
    "def compute_class_accuracy(outputs, targets):\n",
    "    \"\"\"\n",
    "    Compute class-wise accuracy for binary classification.\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): [batch_size, 2], raw logits.\n",
    "        targets (torch.Tensor): [batch_size], long, 0 (bonafide) or 1 (attack).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Correct predictions per class.\n",
    "        torch.Tensor: Total samples per class.\n",
    "    \"\"\"\n",
    "    predicted = torch.argmax(outputs, dim=1)  # [batch_size]\n",
    "    correct_per_class = torch.zeros(2, device=targets.device)\n",
    "    total_per_class = torch.zeros(2, device=targets.device)\n",
    "    for i in range(2):\n",
    "        mask = targets == i\n",
    "        correct_per_class[i] = (predicted[mask] == targets[mask]).sum().float()\n",
    "        total_per_class[i] = mask.sum().float()\n",
    "    return correct_per_class, total_per_class\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    criterion,\n",
    "    device,\n",
    "    num_epochs=30,\n",
    "    early_stop_patience=3,\n",
    "    save_dir=\"model_save_path\",\n",
    "    best_model_pth=\"best_model.pth\",\n",
    "    last_model_pth=\"last_model.pth\",\n",
    "    save_training_metrics_plot=\"training_plot.png\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the ArcFace-based UNetBinaryClassifier model for Replay-Attack.\n",
    "\n",
    "    Args:\n",
    "        model: UNetBinaryClassifier model (from Cell 1).\n",
    "        train_loader: DataLoader for training data (21504 samples, 32x32 images).\n",
    "        val_loader: DataLoader for validation data (8598 samples, 32x32 images).\n",
    "        optimizer: Optimizer (e.g., Adam).\n",
    "        scheduler: Learning rate scheduler (e.g., ReduceLROnPlateau).\n",
    "        criterion: Loss function (e.g., CrossEntropyLoss).\n",
    "        device: Device to train on (cuda or cpu).\n",
    "        num_epochs (int): Number of epochs to train.\n",
    "        early_stop_patience (int): Patience for early stopping.\n",
    "        save_dir (str): Directory to save models and plots.\n",
    "        best_model_pth (str): Filename for best model checkpoint.\n",
    "        last_model_pth (str): Filename for last model checkpoint.\n",
    "        save_training_metrics_plot (str): Filename for training metrics plot.\n",
    "\n",
    "    Returns:\n",
    "        dict: Training history with metrics.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    model = model.to(device)\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"train_f1\": [],\n",
    "        \"val_f1\": [],\n",
    "        \"train_precision\": [],\n",
    "        \"val_precision\": [],\n",
    "        \"train_recall\": [],\n",
    "        \"val_recall\": [],\n",
    "        \"train_class_acc\": [],\n",
    "        \"val_class_acc\": [],\n",
    "        \"train_confusion_matrix\": [],\n",
    "        \"val_confusion_matrix\": [],\n",
    "    }\n",
    "    best_val_loss = float(\"inf\")\n",
    "    early_stop_counter = 0\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    best_model_pth = os.path.join(save_dir, best_model_pth)\n",
    "    last_model_pth = os.path.join(save_dir, last_model_pth)\n",
    "    memory_tracker = MemoryUsage()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        y_true, y_pred = [], []\n",
    "        train_correct_per_class = torch.zeros(2, device=device)\n",
    "        train_total_per_class = torch.zeros(2, device=device)\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(\n",
    "            tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "        ):\n",
    "            inputs, labels = inputs.to(device), labels.to(\n",
    "                device\n",
    "            )  # inputs: [batch_size, 3, 32, 32], labels: [batch_size], long\n",
    "            if not torch.all((labels == 0) | (labels == 1)):\n",
    "                print(f\"Invalid targets: {torch.unique(labels)}\")\n",
    "                break\n",
    "            if torch.isnan(inputs).any() or torch.isinf(inputs).any():\n",
    "                print(f\"Invalid inputs at epoch {epoch}, batch {batch_idx}\")\n",
    "                break\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits, _, _ = model(inputs)  # [batch_size, 2]\n",
    "            loss = criterion(logits, labels)\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"Invalid loss at epoch {epoch}, batch {batch_idx}\")\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted = torch.argmax(logits, dim=1)  # [batch_size]\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            correct_per_class, total_per_class = compute_class_accuracy(logits, labels)\n",
    "            train_correct_per_class += correct_per_class\n",
    "            train_total_per_class += total_per_class\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = correct / total\n",
    "        epoch_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        precision, recall, _, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=\"macro\"\n",
    "        )\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        train_class_acc = (\n",
    "            100.0 * train_correct_per_class / (train_total_per_class + 1e-8)\n",
    "        )\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        val_y_true, val_y_pred = [], []\n",
    "        val_correct_per_class = torch.zeros(2, device=device)\n",
    "        val_total_per_class = torch.zeros(2, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in tqdm(\n",
    "                val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"\n",
    "            ):\n",
    "                val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                logits, _, _ = model(val_inputs)  # [batch_size, 2]\n",
    "                v_loss = criterion(logits, val_labels)\n",
    "                v_predicted = torch.argmax(logits, dim=1)\n",
    "                val_loss += v_loss.item() * val_inputs.size(0)\n",
    "                correct_per_class, total_per_class = compute_class_accuracy(\n",
    "                    logits, val_labels\n",
    "                )\n",
    "                val_correct_per_class += correct_per_class\n",
    "                val_total_per_class += total_per_class\n",
    "                val_correct += (v_predicted == val_labels).sum().item()\n",
    "                val_total += val_labels.size(0)\n",
    "                val_y_true.extend(val_labels.cpu().numpy())\n",
    "                val_y_pred.extend(v_predicted.cpu().numpy())\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        val_f1 = f1_score(val_y_true, val_y_pred, average=\"macro\")\n",
    "        val_precision, val_recall, _, _ = precision_recall_fscore_support(\n",
    "            val_y_true, val_y_pred, average=\"macro\"\n",
    "        )\n",
    "        val_cm = confusion_matrix(val_y_true, val_y_pred)\n",
    "        val_class_acc = 100.0 * val_correct_per_class / (val_total_per_class + 1e-8)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, \"\n",
    "            f\"F1: {epoch_f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Train Class-wise Acc: Class 0: {train_class_acc[0]:.2f}%, Class 1: {train_class_acc[1]:.2f}%\"\n",
    "        )\n",
    "        print(f\"Train Confusion Matrix:\\n{cm}\")\n",
    "        print(\n",
    "            f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}, \"\n",
    "            f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Val Class-wise Acc: Class 0: {val_class_acc[0]:.2f}%, Class 1: {val_class_acc[1]:.2f}%\"\n",
    "        )\n",
    "        print(f\"Val Confusion Matrix:\\n{val_cm}\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        history[\"train_loss\"].append(epoch_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(epoch_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"train_f1\"].append(epoch_f1)\n",
    "        history[\"val_f1\"].append(val_f1)\n",
    "        history[\"train_precision\"].append(precision)\n",
    "        history[\"val_precision\"].append(val_precision)\n",
    "        history[\"train_recall\"].append(recall)\n",
    "        history[\"val_recall\"].append(val_recall)\n",
    "        history[\"train_class_acc\"].append(train_class_acc.tolist())\n",
    "        history[\"val_class_acc\"].append(val_class_acc.tolist())\n",
    "        history[\"train_confusion_matrix\"].append(cm.tolist())\n",
    "        history[\"val_confusion_matrix\"].append(val_cm.tolist())\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        memory_tracker.on_epoch_end()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": (\n",
    "                        scheduler.state_dict() if scheduler else None\n",
    "                    ),\n",
    "                    \"train_loss\": epoch_loss,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"train_acc\": epoch_acc,\n",
    "                    \"val_acc\": val_acc,\n",
    "                    \"train_f1\": epoch_f1,\n",
    "                    \"val_f1\": val_f1,\n",
    "                    \"train_precision\": precision,\n",
    "                    \"val_precision\": val_precision,\n",
    "                    \"train_recall\": recall,\n",
    "                    \"val_recall\": val_recall,\n",
    "                    \"train_class_acc\": train_class_acc.tolist(),\n",
    "                    \"val_class_acc\": val_class_acc.tolist(),\n",
    "                },\n",
    "                best_model_pth,\n",
    "            )\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n",
    "            \"train_loss\": epoch_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"train_acc\": epoch_acc,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"train_f1\": epoch_f1,\n",
    "            \"val_f1\": val_f1,\n",
    "            \"train_precision\": precision,\n",
    "            \"val_precision\": val_precision,\n",
    "            \"train_recall\": recall,\n",
    "            \"val_recall\": val_recall,\n",
    "            \"train_class_acc\": train_class_acc.tolist(),\n",
    "            \"val_class_acc\": val_class_acc.tolist(),\n",
    "        },\n",
    "        last_model_pth,\n",
    "    )\n",
    "\n",
    "    memory_tracker.on_train_end()\n",
    "    training_time = time.time() - start_time\n",
    "    print(\"TRAINING TIME\")\n",
    "    print(f\"--- {training_time:.2f} seconds ---\")\n",
    "\n",
    "    plt.figure(figsize=(12, 16))\n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.plot(history[\"train_acc\"], label=\"Train Acc\")\n",
    "    plt.plot(history[\"val_acc\"], label=\"Val Acc\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.plot(history[\"train_precision\"], label=\"Train Precision\")\n",
    "    plt.plot(history[\"val_precision\"], label=\"Val Precision\")\n",
    "    plt.plot(history[\"train_recall\"], label=\"Train Recall\")\n",
    "    plt.plot(history[\"val_recall\"], label=\"Val Recall\")\n",
    "    plt.title(\"Training and Validation Precision/Recall\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.subplot(4, 1, 4)\n",
    "    plt.plot([x[0] for x in history[\"train_class_acc\"]], label=\"Train Class 0 Acc\")\n",
    "    plt.plot([x[1] for x in history[\"train_class_acc\"]], label=\"Train Class 1 Acc\")\n",
    "    plt.plot([x[0] for x in history[\"val_class_acc\"]], label=\"Val Class 0 Acc\")\n",
    "    plt.plot([x[1] for x in history[\"val_class_acc\"]], label=\"Val Class 1 Acc\")\n",
    "    plt.title(\"Class-wise Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, save_training_metrics_plot))\n",
    "    plt.close()\n",
    "\n",
    "    np.save(os.path.join(save_dir, \"training_metrics.npy\"), history)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08303802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Function to get learning rate for logging\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "\n",
    "# Inspect data distribution\n",
    "def inspect_dataloader(loader, name=\"DataLoader\"):\n",
    "    \"\"\"\n",
    "    Inspect class distribution in a DataLoader.\n",
    "\n",
    "    Args:\n",
    "        loader: DataLoader for training or validation.\n",
    "        name (str): Name of the DataLoader (e.g., 'Training', 'Validation').\n",
    "\n",
    "    Returns:\n",
    "        Counter: Label counts.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for _, lbls in loader:\n",
    "        labels.extend(lbls.cpu().numpy())\n",
    "    label_counts = Counter(labels)\n",
    "    total = len(labels)\n",
    "    print(f\"{name} Class Distribution:\")\n",
    "    print(\n",
    "        f\"Class 0 (Bonafide): {label_counts[0]} ({100 * label_counts[0] / total:.2f}%)\"\n",
    "    )\n",
    "    print(f\"Class 1 (Attack): {label_counts[1]} ({100 * label_counts[1] / total:.2f}%)\")\n",
    "    return label_counts\n",
    "\n",
    "\n",
    "# Inspect train and validation loaders\n",
    "print(\"Inspecting Data Loaders...\")\n",
    "train_label_counts = inspect_dataloader(train_loader, \"Training\")\n",
    "val_label_counts = inspect_dataloader(val_loader, \"Validation\")\n",
    "\n",
    "# Initialize model and training components\n",
    "model = UNetBinaryClassifier(\n",
    "    in_channels=3, base_channels=32, embedding_size=512, num_classes=2\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=3\n",
    ")\n",
    "\n",
    "# Verify input normalization\n",
    "sample_batch, sample_labels = next(iter(train_loader))\n",
    "print(\"Input range:\", sample_batch.min().item(), \"to\", sample_batch.max().item())\n",
    "if sample_batch.max() > 1.0 or sample_batch.min() < -1.0:\n",
    "    print(\"Warning: Inputs are not normalized to [-1, 1].\")\n",
    "\n",
    "# Verify model output shape for a batch\n",
    "sample_batch = torch.randn(32, 3, 128, 128).to(device)  # Batch size 32, 32x32 images\n",
    "with torch.no_grad():\n",
    "    logits, _, _ = model(sample_batch)\n",
    "    print(\"Model output shape for batch:\", logits.shape)  # Expected: [32, 2]\n",
    "\n",
    "# Train the model with custom save paths\n",
    "history = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    num_epochs=50,\n",
    "    early_stop_patience=3,\n",
    "    save_dir=\"model_save_path\",\n",
    "    best_model_pth=\"model_best_checkpoint.pth\",\n",
    "    last_model_pth=\"model_last_checkpoint.pth\",\n",
    "    save_training_metrics_plot=\"training_metrics.png\",\n",
    ")\n",
    "\n",
    "# Visualize sample images to check for data leakage\n",
    "sample_batch, sample_labels = next(iter(val_loader))\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    img = sample_batch[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5  # Denormalize\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Label: {sample_labels[i].item()}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.savefig(\n",
    "    os.path.join(\n",
    "        \"images_save_path\",\n",
    "        \"/validation_images.png\",\n",
    "    )\n",
    ")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import brentq\n",
    "import itertools\n",
    "import os\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "log_dir = \"log_dir_path\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, f\"eval_{int(time())}.log\")\n",
    "logging.basicConfig(\n",
    "    filename=log_file, level=logging.INFO, format=\"%(asctime)s - %(message)s\"\n",
    ")\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "# Load the saved model\n",
    "checkpoint_path = \"model_best_checkpoint.pth\"\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"Checkpoint file not found at {checkpoint_path}\")\n",
    "    raise\n",
    "\n",
    "# Initialize model\n",
    "model = UNetBinaryClassifier(\n",
    "    in_channels=3, base_channels=32, embedding_size=512, num_classes=2\n",
    ").to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    logging.info(f\"Loaded checkpoint from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "    val_acc = checkpoint.get(\"val_acc\", \"unknown\")\n",
    "    if isinstance(val_acc, (int, float)):\n",
    "        logging.info(f\"Validation accuracy: {val_acc:.3f}%\")\n",
    "    else:\n",
    "        logging.info(f\"Validation accuracy: {val_acc}\")\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)\n",
    "    logging.info(\"Loaded model state dictionary\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Collect predictions and true labels for evaluation\n",
    "y_score = []\n",
    "y_pred = []\n",
    "y_true = []\n",
    "all_images = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, labels) in enumerate(val_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.view(-1).float()\n",
    "\n",
    "        # Validate inputs and labels\n",
    "        if inputs.shape[1] != 3 or inputs.shape[2:] != (128, 128):\n",
    "            logging.error(f\"Invalid input shape at batch {batch_idx}: {inputs.shape}\")\n",
    "            break\n",
    "        if not torch.all((labels == 0) | (labels == 1)):\n",
    "            logging.error(\n",
    "                f\"Invalid labels at batch {batch_idx}: {torch.unique(labels)}\"\n",
    "            )\n",
    "            break\n",
    "        if torch.isnan(inputs).any() or torch.isinf(inputs).any():\n",
    "            logging.error(f\"NaN/Inf in inputs at batch {batch_idx}\")\n",
    "            break\n",
    "\n",
    "        # Forward pass (model returns logits, probs, embeddings)\n",
    "        outputs, probs, _ = model(\n",
    "            inputs\n",
    "        )  # outputs: [batch_size, 2], probs: [batch_size, 2]\n",
    "        if probs is None:\n",
    "            probs = torch.softmax(outputs, dim=1)  # Compute softmax if not returned\n",
    "        probs = probs[:, 1]  # Probability for attack class (class 1)\n",
    "        preds = (probs >= 0.5).float()  # Threshold at 0.5\n",
    "\n",
    "        if torch.isnan(probs).any() or torch.isinf(probs).any():\n",
    "            logging.error(f\"NaN/Inf in outputs at batch {batch_idx}\")\n",
    "            break\n",
    "\n",
    "        y_score.extend(probs.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Collect images and labels for t-SNE\n",
    "        all_images.append(inputs.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "# Convert lists to tensors for t-SNE\n",
    "all_images = torch.cat(all_images, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "# Convert evaluation results to numpy arrays\n",
    "y_score = np.array(y_score)\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "\n",
    "# Compute EER\n",
    "def compute_eer(y_true, y_score):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = brentq(lambda x: 1.0 - x - interp1d(fpr, fnr)(x), 0.0, 1.0)\n",
    "    eer = interp1d(fpr, fnr)(eer_threshold)\n",
    "    return eer\n",
    "\n",
    "\n",
    "# Compute HTER\n",
    "def compute_hter(y_true, y_pred, y_score, threshold=0.5):\n",
    "    far = np.sum((y_pred == 1) & (y_true == 0)) / (np.sum(y_true == 0) + 1e-8)\n",
    "    frr = np.sum((y_pred == 0) & (y_true == 1)) / (np.sum(y_true == 1) + 1e-8)\n",
    "\n",
    "    hter = (far + frr) / 2\n",
    "    return hter, far, frr\n",
    "\n",
    "\n",
    "# Confusion Matrix Function\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title=\"Confusion Matrix\"):\n",
    "    plt.figure(figsize=(6, 6), dpi=80)\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / (cm.sum(axis=1)[:, np.newaxis] + 1e-8)\n",
    "        logging.info(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        logging.info(\"Confusion matrix, without normalization\")\n",
    "\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    fname = os.path.join(log_dir, f\"replayattack_confusion_matrix_{int(time())}.png\")\n",
    "    plt.savefig(fname)\n",
    "    logging.info(f\"Saved confusion matrix to {fname}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# t-SNE Visualization for all validation images at once\n",
    "def plot_tsne_validation_embeddings(model, images, labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        # Process all images at once\n",
    "        _, _, embeddings = model(images)\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)  # Shape: [num_samples, 2]\n",
    "\n",
    "    # Plot t-SNE embeddings\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(\n",
    "        embeddings_2d[labels == 1, 0],\n",
    "        embeddings_2d[labels == 1, 1],\n",
    "        c=\"red\",\n",
    "        label=\"Attack\",\n",
    "        alpha=0.6,\n",
    "    )\n",
    "    plt.scatter(\n",
    "        embeddings_2d[labels == 0, 0],\n",
    "        embeddings_2d[labels == 0, 1],\n",
    "        c=\"blue\",\n",
    "        label=\"Bonafide\",\n",
    "        alpha=0.6,\n",
    "    )\n",
    "    plt.title(\n",
    "        \"t-SNE Visualization of Validation Embeddings (Replay-Attack, AAM-loss)\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot\n",
    "    save_path = os.path.join(log_dir, \"tsne_validation_embeddings.png\")\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    logging.info(f\"Saved t-SNE plot to: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Classification Report\n",
    "class_names = [\"Bonafide\", \"Attack\"]\n",
    "logging.info(\"\\nClassification Report:\")\n",
    "report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "logging.info(\"\\n\" + report)\n",
    "\n",
    "# Confusion Matrix\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=False)\n",
    "plot_confusion_matrix(\n",
    "    cnf_matrix, classes=class_names, normalize=True, title=\"Normalized Confusion Matrix\"\n",
    ")\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(5, 5), dpi=80)\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (Replay-Attack)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "fname = os.path.join(log_dir, f\"replayattack_roc_curve_{int(time())}.png\")\n",
    "plt.savefig(fname)\n",
    "logging.info(f\"Saved ROC curve to {fname}\")\n",
    "plt.show()\n",
    "\n",
    "# EER and HTER\n",
    "eer = compute_eer(y_true, y_score)\n",
    "hter, far, frr = compute_hter(y_true, y_pred, y_score)\n",
    "logging.info(f\"FAR : {far:.4f}\")\n",
    "logging.info(f\"FRR: {frr:.4f}\")\n",
    "logging.info(f\"Equal Error Rate (EER): {eer:.4f}\")\n",
    "logging.info(f\"Half Total Error Rate (HTER): {hter:.4f}\")\n",
    "\n",
    "# Score Distribution\n",
    "plt.figure(figsize=(8, 6), dpi=80)\n",
    "plt.hist(y_score[y_true == 0], bins=50, alpha=0.5, label=\"Bonafide\", color=\"blue\")\n",
    "plt.hist(y_score[y_true == 1], bins=50, alpha=0.5, label=\"Attack\", color=\"red\")\n",
    "plt.xlabel(\"Prediction Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Score Distribution (Replay-Attack)\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "fname = os.path.join(log_dir, f\"replayattack_score_distribution_{int(time())}.png\")\n",
    "plt.savefig(fname)\n",
    "logging.info(f\"Saved score distribution to {fname}\")\n",
    "plt.show()\n",
    "\n",
    "# t-SNE Visualization (commented out as in 3DMAD)\n",
    "# plot_tsne_validation_embeddings(model, all_images, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754e0c14",
   "metadata": {},
   "source": [
    "# Attention Map generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e6c6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# === Path Configuration ===\n",
    "BASE_PATH = r\"Your_dataset_path\"\n",
    "IMAGES_DIR = \"images\"\n",
    "ATTACK_DIR = os.path.join(IMAGES_DIR, \"attack_training\")\n",
    "BONAFIDE_DIR = os.path.join(IMAGES_DIR, \"bonifade_validation\")\n",
    "ATTENTION_MAPS_DIR = os.path.join(IMAGES_DIR, \"attention_maps/experimental\")\n",
    "\n",
    "\n",
    "# === Heatmap Utility ===\n",
    "def generate_heatmap(feature_map):\n",
    "    \"\"\"\n",
    "    Generate a heatmap from a feature map by averaging across channels.\n",
    "\n",
    "    Args:\n",
    "        feature_map (torch.Tensor): Feature map of shape [1, C, H, W].\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Heatmap with JET colormap, shape [H, W, 3].\n",
    "    \"\"\"\n",
    "    heatmap = torch.mean(feature_map.squeeze(0), dim=0).cpu().numpy()\n",
    "    heatmap = cv2.normalize(heatmap, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    heatmap = np.uint8(heatmap)\n",
    "    return cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "\n",
    "def overlay_heatmap(image_path, model, transform):\n",
    "    \"\"\"\n",
    "    Overlay a heatmap on the original image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        model: UNetBinaryClassifier model with get_activation_map method.\n",
    "        transform: torchvision transforms for preprocessing.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Image with heatmap overlay, shape [128, 128, 3].\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "    feature_map = model.get_activation_map(input_tensor)\n",
    "    heatmap = generate_heatmap(feature_map)\n",
    "    original = np.array(image.resize((128, 128)))\n",
    "    overlayed = cv2.addWeighted(original, 0.7, heatmap, 0.3, 0)\n",
    "    return overlayed\n",
    "\n",
    "\n",
    "# === t-SNE Visualization ===\n",
    "def plot_tsne_embeddings(model, attack_images, bonafide_images, transform):\n",
    "    \"\"\"\n",
    "    Plot t-SNE visualization of hypersphere embeddings for attack and bonafide images.\n",
    "\n",
    "    Args:\n",
    "        model: UNetBinaryClassifier model.\n",
    "        attack_images (list): List of paths to attack images.\n",
    "        bonafide_images (list): List of paths to bonafide images.\n",
    "        transform: torchvision transforms for preprocessing.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    # Process attack images\n",
    "    for img_path in attack_images:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        input_tensor = transform(image).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            _, _, emb = model(input_tensor)\n",
    "        embeddings.append(emb.squeeze().cpu().numpy())\n",
    "        labels.append(1)  # 1 for attack\n",
    "\n",
    "    # Process bonafide images\n",
    "    for img_path in bonafide_images:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        input_tensor = transform(image).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            _, _, emb = model(input_tensor)\n",
    "        embeddings.append(emb.squeeze().cpu().numpy())\n",
    "        labels.append(0)  # 0 for bonafide\n",
    "\n",
    "    # Convert embeddings and labels to numpy arrays\n",
    "    embeddings = np.array(embeddings)  # Shape: [num_samples, 256]\n",
    "    labels = np.array(labels)  # Shape: [num_samples]\n",
    "\n",
    "    # Apply t-SNE to reduce dimensionality to 2D\n",
    "    tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)  # Shape: [num_samples, 2]\n",
    "\n",
    "    # Plot t-SNE embeddings\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(\n",
    "        embeddings_2d[labels == 1, 0],\n",
    "        embeddings_2d[labels == 1, 1],\n",
    "        c=\"red\",\n",
    "        label=\"Attack\",\n",
    "        alpha=0.6,\n",
    "        s=200,\n",
    "    )\n",
    "    plt.scatter(\n",
    "        embeddings_2d[labels == 0, 0],\n",
    "        embeddings_2d[labels == 0, 1],\n",
    "        c=\"blue\",\n",
    "        label=\"Bonafide\",\n",
    "        alpha=0.6,\n",
    "        s=200,\n",
    "    )\n",
    "    plt.title(\n",
    "        \"t-SNE Visualization of Hypersphere Embeddings (Replay-Attack, ArcFace)\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    plt.xlabel(\"t-SNE Dimension 1\", fontsize=24)\n",
    "    plt.ylabel(\"t-SNE Dimension 2\", fontsize=24)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the plot\n",
    "    os.makedirs(os.path.join(BASE_PATH, ATTENTION_MAPS_DIR), exist_ok=True)\n",
    "    save_path = os.path.join(\n",
    "        BASE_PATH, ATTENTION_MAPS_DIR, \"replayattack_tsne_hypersphere_embeddings.png\"\n",
    "    )\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    print(f\"Saved t-SNE plot to: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# === Main Execution ===\n",
    "def main():\n",
    "    # Model and image paths\n",
    "    model_path = os.path.join(\n",
    "        BASE_PATH, IMAGES_DIR, \"replayattack_model_best_checkpoint.pth\"\n",
    "    )\n",
    "\n",
    "    def get_image_paths(base_path, sub_dir, filenames):\n",
    "        return [os.path.join(base_path, sub_dir, fname) for fname in filenames]\n",
    "\n",
    "    # Add the required paths below\n",
    "    attack_filenames = [\n",
    "        \"File_name.jpg\",\n",
    "    ]\n",
    "\n",
    "    bonafide_filenames = [\n",
    "        \"file_namejpg\",\n",
    "    ]\n",
    "\n",
    "    attack_images = get_image_paths(BASE_PATH, ATTACK_DIR, attack_filenames)\n",
    "    bonafide_images = get_image_paths(BASE_PATH, BONAFIDE_DIR, bonafide_filenames)\n",
    "\n",
    "    image_paths = attack_images + bonafide_images\n",
    "\n",
    "    # Load model\n",
    "    model = UNetBinaryClassifier(\n",
    "        in_channels=3, base_channels=32, embedding_size=512, num_classes=2\n",
    "    )\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device(\"cpu\"))\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "\n",
    "    # Define transform\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((128, 128)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Generating and saving individual attention maps\n",
    "    os.makedirs(os.path.join(BASE_PATH, ATTENTION_MAPS_DIR), exist_ok=True)\n",
    "    for idx, path in enumerate(image_paths):\n",
    "        result = overlay_heatmap(path, model, transform)\n",
    "        save_path = os.path.join(\n",
    "            BASE_PATH, ATTENTION_MAPS_DIR, f\"replayattack_overlay_{idx+1}.png\"\n",
    "        )\n",
    "        cv2.imwrite(save_path, cv2.cvtColor(result, cv2.COLOR_RGB2BGR))\n",
    "        print(f\"Saved: {save_path}\")\n",
    "\n",
    "    # Ploting 3x3 grids for attack and bonafide images\n",
    "    def plot_grid_with_probs(model, image_paths, transform, title, start_idx):\n",
    "        fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            if i >= len(image_paths):\n",
    "                ax.axis(\"off\")\n",
    "                continue\n",
    "\n",
    "            image_path = image_paths[i]\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            input_tensor = transform(image).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_tensor)\n",
    "                if isinstance(logits, tuple):\n",
    "                    logits = logits[0]\n",
    "                probs = torch.softmax(logits, dim=1).squeeze().numpy()\n",
    "\n",
    "            feature_map = model.get_activation_map(input_tensor)\n",
    "            heatmap = generate_heatmap(feature_map)\n",
    "            original = np.array(image.resize((128, 128)))\n",
    "            overlayed = cv2.addWeighted(original, 0.7, heatmap, 0.3, 0)\n",
    "\n",
    "            overlayed_rgb = cv2.cvtColor(overlayed, cv2.COLOR_BGR2RGB)\n",
    "            ax.imshow(overlayed_rgb)\n",
    "            label = \"Attack\" if probs[1] > probs[0] else \"Bonafide\"\n",
    "            prob = probs[1] if label == \"Attack\" else probs[0]\n",
    "            ax.set_title(f\"{label} ({prob:.2f})\", fontsize=9)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        return fig\n",
    "\n",
    "    # Plot attack and bonafide grids\n",
    "    attack_fig = plot_grid_with_probs(\n",
    "        model, attack_images[:9], transform, \"Attack (Replay-Attack)\", 0\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    attack_save_path = os.path.join(BASE_PATH, IMAGES_DIR, \"attack_grid.png\")\n",
    "    attack_fig.savefig(attack_save_path)\n",
    "    print(f\"Saved: {attack_save_path}\")\n",
    "\n",
    "    bonafide_fig = plot_grid_with_probs(\n",
    "        model, bonafide_images[:9], transform, \"Bonafide (Replay-Attack)\", 0\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    bonafide_save_path = os.path.join(BASE_PATH, IMAGES_DIR, \"bonafide_grid.png\")\n",
    "    bonafide_fig.savefig(bonafide_save_path)\n",
    "    print(f\"Saved: {bonafide_save_path}\")\n",
    "\n",
    "    # Combine both grids into a single figure\n",
    "    combined_fig, combined_axs = plt.subplots(1, 2, figsize=(18, 9))\n",
    "    combined_fig.suptitle(\n",
    "        \"Attention Map of Replay-Attack RGB Images: Attack vs Bonafide (ArcFace)\",\n",
    "        fontsize=18,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    attack_img = plt.imread(attack_save_path)\n",
    "    bonafide_img = plt.imread(bonafide_save_path)\n",
    "\n",
    "    combined_axs[0].imshow(attack_img)\n",
    "    combined_axs[0].axis(\"off\")\n",
    "    combined_axs[1].imshow(bonafide_img)\n",
    "    combined_axs[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    combined_save_path = os.path.join(\n",
    "        BASE_PATH, ATTENTION_MAPS_DIR, \"replayattack_final_3x3_grid.png\"\n",
    "    )\n",
    "    combined_fig.savefig(combined_save_path)\n",
    "    print(f\"Saved: {combined_save_path}\")\n",
    "\n",
    "    # # Generate t-SNE plot for hypersphere embeddings\n",
    "    # plot_tsne_embeddings(model, attack_images, bonafide_images, transform)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
